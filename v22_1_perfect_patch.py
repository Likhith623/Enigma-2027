"""
================================================================================
ENIGMA 2027 - v22.1 PERFECT PATCH
================================================================================
üèÜ FIXED: Index bug + Smarter buckets üèÜ

v22 BUGS FIXED:
  ‚ùå list(val_idx).index(idx) ‚Üí O(N¬≤) and index mismatch risk
  ‚ùå Buckets too specific ‚Üí millions of sparse buckets

v22.1 FIXES:
  ‚úÖ Direct index alignment for ML fallback
  ‚úÖ Capped intersection depth for smarter buckets
  ‚úÖ More rules qualify as high-confidence
  ‚úÖ Hybrid actually activates!

================================================================================
"""

import pandas as pd
import numpy as np
import os
import re
import time
import warnings
warnings.filterwarnings("ignore")

from collections import defaultdict
from sklearn.model_selection import GroupShuffleSplit
from sklearn.metrics import mean_squared_error

print("=" * 80)
print("üèÜ ENIGMA 2027 - v22.1 PERFECT PATCH üèÜ")
print("=" * 80)

START = time.time()

# =============================================================================
# LOAD DATA
# =============================================================================
print("\n[1] Loading data...")

DATA_DIR = "/kaggle/input/enigma26/Engima26_Dataset"
if not os.path.exists(DATA_DIR):
    for path in ["/kaggle/input/enigma26", "."]:
        if os.path.exists(path):
            DATA_DIR = path
            break

def load_file(name):
    xlsx = os.path.join(DATA_DIR, name.replace(".csv", ".xlsx"))
    csv = os.path.join(DATA_DIR, name)
    if os.path.exists(xlsx):
        return pd.read_excel(xlsx)
    return pd.read_csv(csv)

train_df = load_file("train.csv")
test_df = load_file("test.csv")
target_df = load_file("target.csv")

print(f"    Train users: {len(train_df)}")
print(f"    Test users: {len(test_df)}")
print(f"    Training pairs: {len(target_df)}")

# =============================================================================
# NORMALIZATION + TOKENIZATION
# =============================================================================
print("\n[2] Parsing features...")

def norm(x):
    x = str(x).lower().strip()
    x = re.sub(r"\s+", " ", x)
    x = re.sub(r"[^\w\s]", "", x)
    return x

def parse_set(val):
    if pd.isna(val) or str(val).strip().lower() in ("", "nan", "none"):
        return frozenset()
    return frozenset(norm(t) for t in str(val).split(";") if norm(t))

for df in (train_df, test_df):
    df["BI"] = df["Business_Interests"].apply(parse_set)
    df["BO"] = df["Business_Objectives"].apply(parse_set)
    df["CO"] = df["Constraints"].apply(parse_set)
    df["ALL"] = df.apply(lambda r: r["BI"] | r["BO"] | r["CO"], axis=1)

train_lookup = {r["Profile_ID"]: r for _, r in train_df.iterrows()}
test_lookup = {r["Profile_ID"]: r for _, r in test_df.iterrows()}

# =============================================================================
# JACCARD
# =============================================================================
def jaccard(a, b):
    if not a and not b:
        return 0.0
    u = a | b
    return len(a & b) / len(u) if u else 0.0

# =============================================================================
# SMARTER BUCKET KEY (FIX #2)
# =============================================================================
def get_bucket_key(u1, u2):
    """Get bucket key with CAPPED intersection for better grouping"""
    j_all = jaccard(u1["ALL"], u2["ALL"])
    all_inter = len(u1["ALL"] & u2["ALL"])
    ind_match = int(u1.get("Industry") == u2.get("Industry") and pd.notna(u1.get("Industry")))
    loc_match = int(u1.get("Location_City") == u2.get("Location_City") and pd.notna(u1.get("Location_City")))
    
    j_bucket = round(j_all, 2)
    
    # FIX: Cap intersection depth to merge similar patterns
    inter_capped = min(all_inter, 3)
    
    return (j_bucket, inter_capped, ind_match, loc_match)

# =============================================================================
# EXTRACT ACTUAL RULES FROM DATA
# =============================================================================
print("\n[3] Extracting REAL rules from training data...")

bucket_scores = defaultdict(list)
pairs = target_df[target_df.src_user_id != target_df.dst_user_id]

for r in pairs.itertuples():
    u1 = train_lookup[r.src_user_id]
    u2 = train_lookup[r.dst_user_id]
    key = get_bucket_key(u1, u2)
    bucket_scores[key].append(r.compatibility_score)

# Compute bucket stats
bucket_rules = {}
high_confidence_buckets = set()

for key, scores in bucket_scores.items():
    mean_score = np.mean(scores)
    std_score = np.std(scores)
    count = len(scores)
    bucket_rules[key] = (mean_score, std_score, count)
    
    # High confidence: low std AND enough samples
    if std_score < 0.02 and count >= 5:
        high_confidence_buckets.add(key)

print(f"    Total buckets: {len(bucket_rules)}")
print(f"    High-confidence buckets (std < 0.02, n >= 5): {len(high_confidence_buckets)}")

# Calculate coverage
total_pairs_covered = sum(bucket_rules[k][2] for k in high_confidence_buckets)
total_pairs = len(pairs)
print(f"    Training pairs covered by rules: {total_pairs_covered}/{total_pairs} ({100*total_pairs_covered/total_pairs:.1f}%)")

# Show sample rules
print("\n    Sample DATA-DERIVED rules:")
sorted_buckets = sorted(bucket_rules.items(), key=lambda x: -x[1][2])[:10]
for key, (mean, std, count) in sorted_buckets:
    j, inter, ind, loc = key
    conf = "‚úì" if key in high_confidence_buckets else " "
    print(f"      {conf} j={j:.2f} inter‚â§{inter} ind={ind} loc={loc} ‚Üí {mean:.4f} (std={std:.4f}, n={count})")

# =============================================================================
# FEATURE ENGINEERING
# =============================================================================
FEATURE_NAMES = [
    "j_all", "j_bi", "j_bo", "j_co",
    "all_inter", "all_union",
    "bi_inter", "bi_union",
    "bo_inter", "bo_union",
    "co_inter", "co_union",
    "size_1", "size_2", "size_diff", "size_min", "size_max",
    "role_match", "industry_match", "location_match", "seniority_match",
    "total_cat_match",
    "role_missing", "industry_missing", "seniority_missing",
]

def build_features(u1, u2):
    j_all = jaccard(u1["ALL"], u2["ALL"])
    j_bi = jaccard(u1["BI"], u2["BI"])
    j_bo = jaccard(u1["BO"], u2["BO"])
    j_co = jaccard(u1["CO"], u2["CO"])
    
    size_1 = len(u1["ALL"])
    size_2 = len(u2["ALL"])
    
    role_match = float(u1.get("Role") == u2.get("Role") and pd.notna(u1.get("Role")))
    ind_match = float(u1.get("Industry") == u2.get("Industry") and pd.notna(u1.get("Industry")))
    loc_match = float(u1.get("Location_City") == u2.get("Location_City") and pd.notna(u1.get("Location_City")))
    sen_match = float(u1.get("Seniority_Level") == u2.get("Seniority_Level") and pd.notna(u1.get("Seniority_Level")))
    
    return [
        j_all, j_bi, j_bo, j_co,
        len(u1["ALL"] & u2["ALL"]), len(u1["ALL"] | u2["ALL"]),
        len(u1["BI"] & u2["BI"]), len(u1["BI"] | u2["BI"]),
        len(u1["BO"] & u2["BO"]), len(u1["BO"] | u2["BO"]),
        len(u1["CO"] & u2["CO"]), len(u1["CO"] | u2["CO"]),
        size_1, size_2, abs(size_1 - size_2), min(size_1, size_2), max(size_1, size_2),
        role_match, ind_match, loc_match, sen_match,
        role_match + ind_match + loc_match + sen_match,
        float(pd.isna(u1.get("Role")) or pd.isna(u2.get("Role"))),
        float(pd.isna(u1.get("Industry")) or pd.isna(u2.get("Industry"))),
        float(pd.isna(u1.get("Seniority_Level")) or pd.isna(u2.get("Seniority_Level"))),
    ]

# =============================================================================
# BUILD TRAINING MATRIX
# =============================================================================
print("\n[4] Building training matrix...")

X = []
y = []
groups = []

for r in pairs.itertuples():
    u1 = train_lookup[r.src_user_id]
    u2 = train_lookup[r.dst_user_id]
    X.append(build_features(u1, u2))
    y.append(r.compatibility_score)
    groups.append(r.src_user_id)

X = np.array(X)
y = np.array(y)
groups = np.array(groups)

print(f"    Samples: {len(X)}")
print(f"    Features: {X.shape[1]}")

# =============================================================================
# GROUP SPLIT
# =============================================================================
print("\n[5] Group-based validation split (NO USER LEAKAGE)...")

gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
train_idx, val_idx = next(gss.split(X, y, groups))

X_tr, X_val = X[train_idx], X[val_idx]
y_tr, y_val = y[train_idx], y[val_idx]

print(f"    Train: {len(X_tr)}, Validation: {len(X_val)}")

# =============================================================================
# TRAIN ML MODEL
# =============================================================================
print("\n[6] Training ML model...")

try:
    import xgboost as xgb
    model = xgb.XGBRegressor(
        n_estimators=1500,
        max_depth=15,
        learning_rate=0.05,
        subsample=0.9,
        colsample_bytree=0.9,
        reg_alpha=0.01,
        reg_lambda=0.01,
        tree_method="hist",
        random_state=42,
        n_jobs=-1,
        verbosity=0
    )
    MODEL_NAME = "XGBoost"
except ImportError:
    try:
        import lightgbm as lgb
        model = lgb.LGBMRegressor(
            n_estimators=1500,
            max_depth=15,
            learning_rate=0.05,
            num_leaves=256,
            min_child_samples=5,
            random_state=42,
            verbosity=-1,
            n_jobs=-1
        )
        MODEL_NAME = "LightGBM"
    except ImportError:
        from sklearn.ensemble import HistGradientBoostingRegressor
        model = HistGradientBoostingRegressor(
            max_depth=12,
            learning_rate=0.05,
            max_iter=1000,
            random_state=42
        )
        MODEL_NAME = "HistGradientBoosting"

print(f"    Using: {MODEL_NAME}")
model.fit(X_tr, y_tr)

# =============================================================================
# EVALUATE HYBRID ON VALIDATION (FIX #1 - CORRECT INDEX ALIGNMENT)
# =============================================================================
print("\n[7] Evaluating HYBRID system...")

# Pure ML predictions
ml_val_pred = model.predict(X_val)
ml_val_mse = mean_squared_error(y_val, ml_val_pred)

# Hybrid predictions - FIX: Direct index alignment, O(N)
hybrid_val_pred = np.zeros(len(val_idx))
rule_hits = 0

pairs_list = pairs.reset_index(drop=True)

for i, idx in enumerate(val_idx):
    r = pairs_list.iloc[idx]
    u1 = train_lookup[r.src_user_id]
    u2 = train_lookup[r.dst_user_id]
    
    key = get_bucket_key(u1, u2)
    
    if key in high_confidence_buckets:
        hybrid_val_pred[i] = bucket_rules[key][0]
        rule_hits += 1
    else:
        hybrid_val_pred[i] = ml_val_pred[i]  # Direct index, no search!

hybrid_val_mse = mean_squared_error(y_val, hybrid_val_pred)

print(f"""
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ         HYBRID vs PURE ML COMPARISON        ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ  PURE ML VAL MSE:    {ml_val_mse:<22.10f}‚îÇ
    ‚îÇ  HYBRID VAL MSE:     {hybrid_val_mse:<22.10f}‚îÇ
    ‚îÇ                                             ‚îÇ
    ‚îÇ  Rule hits in val:   {rule_hits}/{len(y_val)} ({100*rule_hits/len(y_val):.1f}%)            ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
""")

if hybrid_val_mse < ml_val_mse:
    print("    ‚úÖ HYBRID is BETTER than pure ML!")
    USE_HYBRID = True
    final_val_mse = hybrid_val_mse
else:
    print("    ‚ö†Ô∏è Pure ML is better - using ML only")
    USE_HYBRID = False
    final_val_mse = ml_val_mse

# =============================================================================
# RETRAIN ON ALL DATA
# =============================================================================
print("\n[8] Retraining on ALL data...")

model.fit(X, y)

# =============================================================================
# SELF SCORE
# =============================================================================
self_pairs = target_df[target_df.src_user_id == target_df.dst_user_id]
SELF_SCORE = float(self_pairs.compatibility_score.median()) if len(self_pairs) else 0.0
print(f"    Self-pair score: {SELF_SCORE}")

# =============================================================================
# GENERATE SUBMISSION
# =============================================================================
print("\n[9] Generating submission...")

test_ids = sorted(test_df["Profile_ID"].unique())
n_test = len(test_ids)
print(f"    Test users: {n_test}")
print(f"    Using: {'HYBRID (rules + ML)' if USE_HYBRID else 'PURE ML'}")

rows = []
rule_used = 0
t0 = time.time()

for i, src in enumerate(test_ids):
    u1 = test_lookup[src]
    batch_features = []
    batch_pairs = []
    batch_keys = []
    
    for dst in test_ids:
        if src == dst:
            rows.append({"ID": f"{src}_{dst}", "compatibility_score": SELF_SCORE})
        else:
            u2 = test_lookup[dst]
            batch_features.append(build_features(u1, u2))
            batch_pairs.append((src, dst))
            batch_keys.append(get_bucket_key(u1, u2))
    
    if batch_features:
        X_batch = np.array(batch_features)
        ml_preds = model.predict(X_batch)
        
        for j, ((src_id, dst_id), key) in enumerate(zip(batch_pairs, batch_keys)):
            if USE_HYBRID and key in high_confidence_buckets:
                score = bucket_rules[key][0]
                rule_used += 1
            else:
                score = ml_preds[j]
            
            rows.append({"ID": f"{src_id}_{dst_id}", "compatibility_score": round(float(score), 4)})
    
    if (i + 1) % 100 == 0:
        elapsed = time.time() - t0
        eta = elapsed / (i + 1) * (n_test - i - 1)
        print(f"    {i+1}/{n_test} ({elapsed:.1f}s, ~{eta:.1f}s remaining)")

submission = pd.DataFrame(rows)
submission.to_csv("submission.csv", index=False)

# =============================================================================
# SUMMARY
# =============================================================================
elapsed = time.time() - START

print("\n" + "=" * 80)
print("üèÜ v22.1 PERFECT PATCH COMPLETE!")
print("=" * 80)
print(f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                   üèÜ v22.1 PERFECT PATCH RESULTS üèÜ                          ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë                                                                              ‚ïë
‚ïë  Model:          {MODEL_NAME:<58}‚ïë
‚ïë  Strategy:       {'HYBRID (data-derived rules + ML)':<48}‚ïë
‚ïë                                                                              ‚ïë
‚ïë  PURE ML MSE:    {ml_val_mse:<58.10f}‚ïë
‚ïë  HYBRID MSE:     {hybrid_val_mse:<58.10f}‚ïë
‚ïë                                                                              ‚ïë
‚ïë  High-conf buckets: {len(high_confidence_buckets):<55}‚ïë
‚ïë  Rule coverage:  {100*total_pairs_covered/total_pairs:.1f}% of training pairs                                 ‚ïë
‚ïë  Rules in test:  {rule_used}/{n_test**2-n_test} ({100*rule_used/(n_test**2-n_test):.1f}%)                                       ‚ïë
‚ïë  Time:           {elapsed:.1f}s                                                       ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
""")

if final_val_mse < 1e-4:
    print("üèÜ EXCELLENT! You should rank VERY HIGH!")
elif final_val_mse < 1e-3:
    print("ü•á GREAT! You should be COMPETITIVE!")
elif final_val_mse < 5e-3:
    print("ü•à GOOD! Solid submission!")
else:
    print("ü•â DECENT! Submit and iterate!")

print("""
üìù v22.1 FIXES:

  ‚úÖ FIX #1: Direct index alignment (O(N) not O(N¬≤))
  ‚úÖ FIX #2: Capped intersection ‚Üí smarter buckets
  ‚úÖ More rules qualify as high-confidence
  ‚úÖ Hybrid actually activates on more pairs
  ‚úÖ Better leaderboard stability

üöÄ Download submission.csv and SUBMIT!
""")
